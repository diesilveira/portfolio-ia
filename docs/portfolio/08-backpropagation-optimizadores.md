# TA8 - Backpropagation y Optimizadores: Experimentaci√≥n Sistem√°tica en Deep Learning

## Resumen de la Tarea

Esta tarea se centr√≥ en la experimentaci√≥n sistem√°tica con redes neuronales profundas, explorando diferentes arquitecturas, optimizadores y t√©cnicas de regularizaci√≥n. El objetivo principal fue comprender c√≥mo diferentes hiperpar√°metros y t√©cnicas afectan el rendimiento del modelo, realizando m√°s de 25 experimentos controlados en el dataset CIFAR-10.

### Metodolog√≠a

1. **Preparaci√≥n del dataset**: Carga y preprocesamiento de CIFAR-10 (50,000 im√°genes de entrenamiento, 10,000 de test)
2. **Experimentaci√≥n con arquitecturas**: Prueba de diferentes profundidades, anchos y configuraciones
3. **Optimizaci√≥n de hiperpar√°metros**: Comparaci√≥n de optimizadores (Adam, SGD, RMSprop, AdamW)
4. **T√©cnicas de regularizaci√≥n**: Implementaci√≥n de Dropout, BatchNormalization, L2 regularization
5. **Callbacks avanzados**: EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, Cosine Decay
6. **An√°lisis comparativo**: Evaluaci√≥n sistem√°tica de todos los experimentos

### Conceptos Clave

- **Backpropagation**: Algoritmo fundamental para entrenar redes neuronales mediante propagaci√≥n del error hacia atr√°s
- **Optimizadores**: Algoritmos que ajustan los pesos de la red (Adam, SGD, RMSprop, AdamW)
- **Learning Rate**: Tasa de aprendizaje que controla qu√© tan grandes son los ajustes de pesos
- **Regularizaci√≥n**: T√©cnicas para prevenir overfitting (Dropout, L2, BatchNormalization)
- **Callbacks**: Funciones que se ejecutan durante el entrenamiento para mejorar el proceso

## Implementaci√≥n y Resultados

### Dataset: CIFAR-10


**Clases**: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck

### Parte 1: Experimentaci√≥n con Arquitecturas

#### Comparaci√≥n de Arquitecturas

| Experimento | Arquitectura | Test Accuracy | Par√°metros |
|------------|--------------|---------------|------------|
| Baseline | 2 capas √ó 32 | 47.4% | 99,722 |
| Deep (4√ó64) | 4 capas √ó 64 | 50.0% | 222,986 |
| Wide (3√ó128) | 3 capas √ó 128 | 47.7% | 427,658 |
| Pyramid | 256‚Üí128‚Üí64 | 48.2% | 362,378 |

#### T√©cnicas de Regularizaci√≥n

| T√©cnica | Test Accuracy | Observaci√≥n |
|---------|---------------|-------------|
| GELU activation | 45.9% | Peor que ReLU en este caso |
| Tanh activation | 47.7% | Similar a baseline |
| BatchNormalization | 50.3% | ‚úÖ Mejora significativa |
| Dropout (0.3) | 51.8% | ‚úÖ Reduce overfitting |
| L2 Regularization | 49.0% | Mejora moderada |
| He Normal Init | 47.2% | Sin mejora notable |
| **Combinaci√≥n** | **52.4%** | ‚úÖ **Mejor arquitectura** |

**Modelo combinado √≥ptimo**:
```python
model_combined = keras.Sequential([
    layers.Dense(256, kernel_initializer='he_normal', 
                 kernel_regularizer=regularizers.l2(1e-4)),
    layers.BatchNormalization(),
    layers.Activation('relu'),
    layers.Dropout(0.3),
    
    layers.Dense(128, kernel_initializer='he_normal',
                 kernel_regularizer=regularizers.l2(1e-4)),
    layers.BatchNormalization(),
    layers.Activation('relu'),
    layers.Dropout(0.3),
    
    layers.Dense(64, kernel_initializer='he_normal',
                 kernel_regularizer=regularizers.l2(1e-4)),
    layers.BatchNormalization(),
    layers.Activation('relu'),
    layers.Dropout(0.2),
    
    layers.Dense(10, activation='softmax')
])
```

### Parte 2: Experimentaci√≥n con Optimizadores

#### Comparaci√≥n de Learning Rates (Adam)

| Learning Rate | Test Accuracy | Observaci√≥n |
|--------------|---------------|-------------|
| 0.01 | 52.6% | Converge r√°pido pero inestable |
| 0.001 (default) | 54.5% | ‚úÖ Balance √≥ptimo |
| 0.0001 | 51.2% | Muy lento |

#### Comparaci√≥n de Optimizadores

| Optimizador | Configuraci√≥n | Test Accuracy | Tiempo |
|------------|---------------|---------------|--------|
| Adam (LR=0.001) | Default | 54.5% | 76s |
| SGD + Momentum | momentum=0.9 | 53.8% | 74s |
| SGD + Nesterov | momentum=0.9, nesterov=True | 54.4% | 74s |
| RMSprop | rho=0.9 | 54.9% | 75s |
| **AdamW** | weight_decay=1e-4 | **54.8%** | 76s |

#### Impacto del Batch Size

| Batch Size | Test Accuracy | Tiempo | Observaci√≥n |
|-----------|---------------|--------|-------------|
| 32 | 54.5% | 131s | M√°s lento pero m√°s estable |
| 64 | 54.5% | 76s | ‚úÖ Balance √≥ptimo |
| 128 | 55.2% | 52s | M√°s r√°pido pero menos estable |

### Parte 3: Experimentaci√≥n con Callbacks

#### EarlyStopping

```python
early_stop = callbacks.EarlyStopping(
    monitor='val_loss', 
    patience=5,
    restore_best_weights=True
)
```

**Resultado**: Detuvo en √©poca 23/50 | Test Acc: 54.7%

#### ReduceLROnPlateau

```python
reduce_lr = callbacks.ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=3,
    min_lr=1e-6
)
```

**Resultado**: Test Acc: 55.2% | Redujo LR 3 veces durante entrenamiento

#### Cosine Decay Learning Rate

```python
def cosine_decay(epoch, lr, initial_lr=0.001, epochs=30):
    return min_lr + 0.5 * (initial_lr - min_lr) * \
           (1 + np.cos(np.pi * epoch / epochs))
```

**Resultado**: Test Acc: 56.0% | ‚úÖ Mejora gradual y suave

#### Combinaci√≥n de Callbacks

```python
callbacks_list = [
    EarlyStopping(patience=10),
    ReduceLROnPlateau(factor=0.5, patience=4),
    ModelCheckpoint(save_best_only=True)
]
```

**Resultado**: Test Acc: 56.2% | üèÜ **MEJOR RESULTADO GENERAL**

## Resultados Finales

### Top 5 Experimentos

| Ranking | Experimento | Test Acc | Val Acc | Par√°metros | Tiempo |
|---------|------------|----------|---------|------------|--------|
| ü•á | Combined Callbacks | 56.2% | 57.1% | 830,282 | 163s |
| ü•à | Cosine Decay | 56.0% | 57.3% | 830,282 | 117s |
| ü•â | ReduceLROnPlateau | 55.2% | 56.7% | 830,282 | 108s |
| 4 | Batch Size 128 | 55.2% | 55.5% | 830,282 | 52s |
| 5 | RMSprop | 54.9% | 55.8% | 830,282 | 75s |

### Modelo Ganador: Combined Callbacks

- **Arquitectura**: 256‚Üí128‚Üí64 con BatchNorm + Dropout + L2
- **Optimizador**: Adam (LR=0.001)
- **Callbacks**: EarlyStopping + ReduceLROnPlateau + ModelCheckpoint
- **Training Accuracy**: 74.4%
- **Validation Accuracy**: 57.1%
- **Test Accuracy**: 56.2%
- **Par√°metros totales**: 830,282
- **Mejor √©poca**: 36/50
- **Tiempo de entrenamiento**: 163 segundos

### An√°lisis de Overfitting

El modelo ganador muestra una diferencia de **18.2%** entre training y test accuracy, indicando cierto overfitting. Sin embargo, las t√©cnicas de regularizaci√≥n aplicadas (Dropout, BatchNorm, L2, EarlyStopping) lograron mitigar significativamente este problema comparado con el baseline.

## Reflexi√≥n

### Hallazgos Principales

1. **La combinaci√≥n es clave**: Ninguna t√©cnica individual dio el mejor resultado; la combinaci√≥n de m√∫ltiples t√©cnicas (BatchNorm + Dropout + L2 + Callbacks) fue lo m√°s efectivo

2. **Learning Rate es cr√≠tico**: Un LR muy alto (0.01) causa inestabilidad, muy bajo (0.0001) es demasiado lento. El valor por defecto (0.001) funcion√≥ mejor

3. **Callbacks inteligentes**: Los callbacks adaptativos (ReduceLROnPlateau, Cosine Decay) superaron a los est√°ticos, permitiendo que el modelo se ajuste durante el entrenamiento

4. **Batch size trade-off**: Batch sizes m√°s grandes son m√°s r√°pidos pero menos estables; batch sizes peque√±os son m√°s lentos pero m√°s precisos

5. **Arquitectura profunda vs ancha**: En este caso, una arquitectura piramidal (256‚Üí128‚Üí64) funcion√≥ mejor que arquitecturas uniformes

### Desaf√≠os Encontrados

- **Overfitting persistente**: A pesar de m√∫ltiples t√©cnicas de regularizaci√≥n, el gap entre train y test accuracy sigue siendo significativo
- **Tiempo de experimentaci√≥n**: 25+ experimentos requirieron varias horas de entrenamiento
- **Selecci√≥n de hiperpar√°metros**: Encontrar la combinaci√≥n √≥ptima requiere mucha experimentaci√≥n
- **Limitaciones del MLP**: Las redes densas no son ideales para im√°genes; CNNs ser√≠an m√°s apropiadas

### Comparaciones y Mejoras

- **Baseline vs Mejor modelo**: Mejora de 8.8% en test accuracy (47.4% ‚Üí 56.2%)
- **Adam vs otros optimizadores**: Adam y AdamW fueron consistentemente mejores que SGD
- **Sin callbacks vs con callbacks**: Los callbacks mejoraron 2-3% el rendimiento final
- **Arquitectura simple vs compleja**: M√°s par√°metros no siempre significa mejor rendimiento

## Visualizaci√≥n con TensorBoard

Durante todos los experimentos utilizamos **TensorBoard**, una herramienta de visualizaci√≥n desarrollada por Google para monitorear y analizar el entrenamiento de redes neuronales en tiempo real.

### ¬øPara qu√© sirve TensorBoard?

TensorBoard es fundamental en deep learning porque permite:

- **Monitorear m√©tricas en tiempo real**: Visualizar loss y accuracy durante el entrenamiento sin esperar a que termine
- **Comparar m√∫ltiples experimentos**: Superponer gr√°ficas de diferentes configuraciones para identificar cu√°l funciona mejor
- **Detectar problemas temprano**: Identificar overfitting, underfitting o divergencia antes de desperdiciar tiempo computacional
- **Analizar distribuciones de pesos**: Ver histogramas de c√≥mo evolucionan los pesos y gradientes en cada capa
- **Visualizar la arquitectura**: Inspeccionar el grafo computacional del modelo
- **Debugging eficiente**: Encontrar capas problem√°ticas o configuraciones que causan NaN o explosi√≥n de gradientes

En nuestros experimentos, TensorBoard fue esencial para comparar los 25+ experimentos simult√°neamente, permiti√©ndonos identificar que el modelo "Combined Callbacks" converg√≠a mejor que los dem√°s.


![Visualizaci√≥n de experimentos en TensorBoard](08-imagenes/tensorboard-experiments.png)

*TensorBoard mostrando la comparaci√≥n de loss y accuracy entre m√∫ltiples experimentos. Cada l√≠nea de color representa un experimento diferente, permitiendo identificar visualmente cu√°les configuraciones convergen mejor.*

## Conclusiones

### 1. Lecciones sobre Arquitecturas y Regularizaci√≥n

A trav√©s de los experimentos con diferentes arquitecturas, aprendimos que **la complejidad no garantiza mejor rendimiento**. El modelo baseline con solo 99,722 par√°metros obtuvo 47.4% de accuracy, mientras que arquitecturas m√°s complejas sin regularizaci√≥n adecuada no mejoraron significativamente. La clave estuvo en la **combinaci√≥n de t√©cnicas**:

- **BatchNormalization** estabiliza el entrenamiento y permite learning rates m√°s altos, mejorando de 47.4% a 50.3%
- **Dropout (0.3)** fuerza a la red a aprender representaciones robustas, alcanzando 51.8%
- **L2 Regularization** penaliza pesos grandes, previniendo overfitting moderadamente
- **Arquitectura piramidal (256‚Üí128‚Üí64)** comprime gradualmente las representaciones, similar a CNNs

La combinaci√≥n de BatchNorm + Dropout + L2 + He Normal logr√≥ 52.4%, demostrando que las t√©cnicas de regularizaci√≥n son complementarias.

### 2. Optimizadores y Learning Rate

Los experimentos con 9 configuraciones diferentes revelaron que **el learning rate es m√°s cr√≠tico que el optimizador espec√≠fico**:

- **LR muy alto (0.01)**: Converge r√°pido pero inestable, oscilando alrededor del √≥ptimo
- **LR √≥ptimo (0.001)**: Balance perfecto entre velocidad y estabilidad
- **LR muy bajo (0.0001)**: Demasiado lento, requiere muchas m√°s √©pocas

Entre optimizadores, **Adam y AdamW** fueron consistentemente superiores a SGD. El batch size tambi√©n importa: batch size 128 fue 2.5√ó m√°s r√°pido pero menos estable que batch size 32.

### 3. Callbacks Adaptativos

Los callbacks transformaron el proceso de entrenamiento de est√°tico a adaptativo:

- **EarlyStopping** ahorr√≥ 27 √©pocas innecesarias, deteniendo en √©poca 23/50
- **ReduceLROnPlateau** ajust√≥ el LR 3 veces durante el entrenamiento, alcanzando 55.2%
- **Cosine Decay** proporcion√≥ la reducci√≥n m√°s suave del LR, logrando 56.0%
- **ModelCheckpoint** garantiz√≥ que nunca perdemos el mejor modelo

La combinaci√≥n de EarlyStopping + ReduceLROnPlateau + ModelCheckpoint logr√≥ el **mejor resultado general: 56.2%**, demostrando que el entrenamiento adaptativo supera a las configuraciones est√°ticas.

### 4. El Problema Fundamental: MLPs no son para Im√°genes

A pesar de los experimentos  y la aplicaci√≥n de varias pr√°cticas de deep learning, nuestro mejor modelo alcanz√≥ solo **56.2% de test accuracy** con un **gap de overfitting de 18.2%** (74.4% train vs 56.2% test). Este resultado es una **limitaci√≥n fundamental de las redes densas (MLPs) para procesar im√°genes**.

**¬øPor qu√© las MLPs fallan en im√°genes?**

1. **P√©rdida de estructura espacial**: Al aplanar las im√°genes de 32√ó32√ó3 a vectores de 3,072 dimensiones, destruimos completamente la informaci√≥n espacial. Una MLP no sabe que p√≠xeles vecinos est√°n relacionados.

2. **Falta de invarianza traslacional**: Si un objeto se mueve unos p√≠xeles, la MLP lo ve como una entrada completamente diferente. No puede generalizar patrones aprendidos en una posici√≥n a otras posiciones.

3. **N√∫mero excesivo de par√°metros**: Nuestro mejor modelo tiene 830,282 par√°metros para im√°genes de solo 32√ó32 p√≠xeles. Esto hace que sea extremadamente propenso a overfitting.

4. **No captura jerarqu√≠as visuales**: Las im√°genes tienen estructura jer√°rquica (bordes ‚Üí texturas ‚Üí partes ‚Üí objetos), pero las MLPs tratan todos los p√≠xeles como independientes.

### 5. Reflexi√≥n Final

Este ejercicio de experimentaci√≥n estuvo muy bueno y completo por las lecciones aprendidas sobre el proceso de deep learning:

‚úÖ **Aprendimos a experimentar sistem√°ticamente**: Cambiar una variable a la vez, documentar resultados, comparar m√©tricas

‚úÖ **Dominamos t√©cnicas fundamentales**: BatchNorm, Dropout, optimizadores, callbacks - herramientas que usaremos en cualquier arquitectura

‚úÖ **Entendimos las limitaciones**: Saber cu√°ndo una arquitectura no es apropiada es tan importante como saber cu√°ndo s√≠ lo es

