# TA12 - Segment Anything Model (SAM): Segmentaci√≥n de √Åreas Inundadas

## Resumen de la Tarea

Esta tarea exploramos **Segment Anything Model (SAM)** aplicado a un caso de uso cr√≠tico: segmentaci√≥n de √°reas inundadas para monitoreo de desastres y respuesta a emergencias. El objetivo fue comprender c√≥mo funcionan los modelos de segmentaci√≥n con prompts y evaluar la efectividad del fine-tuning en dominios espec√≠ficos.

### Metodolog√≠a

1. **Baseline Zero-shot**: SAM preentrenado sin modificaciones
2. **Dataset**: Flood Area Segmentation (290 im√°genes de √°reas inundadas)
3. **Prompting**: Evaluaci√≥n con point prompts y box prompts
4. **Fine-tuning**: Adaptar SAM al dominio espec√≠fico de detecci√≥n de agua

SAM es un modelo general entrenado en SA-1B (11M im√°genes, 1.1B m√°scaras), pero ¬øpuede especializarse en detectar agua en contextos de inundaci√≥n sin perder su capacidad de generalizaci√≥n?

## Implementaci√≥n y Resultados

### Parte 1: Exploraci√≥n del Dataset

#### Caracter√≠sticas del dataset

El dataset consta de 290 im√°genes a√©reas/satelitales de √°reas inundadas, divididas en 80 para entrenamiento y 20 para validaci√≥n. Las im√°genes son RGB acompa√±adas de m√°scaras binarias (0=tierra, 1=agua) con tama√±os variables que fueron redimensionadas a 1024√ó1024 para SAM. El dataset est√° balanceado en t√©rminos de p√≠xeles (43% agua vs 57% tierra) con gran variabilidad en tama√±os de imagen y contextos diversos incluyendo escenarios urbanos, rurales, r√≠os e inundaciones costeras. La calidad del dataset es alta con m√°scaras precisas y boundaries bien definidos, proporcionando una buena variedad de escenarios de inundaci√≥n.

### Parte 2: SAM Pretrained - Zero-shot Inference

#### Arquitectura de SAM

- **Image Encoder**: ViT-B (Vision Transformer Base) - 93.7M par√°metros
- **Prompt Encoder**: Procesa points, boxes o masks como input
- **Mask Decoder**: Genera m√°scaras de segmentaci√≥n - 4.1M par√°metros

#### Tipos de prompts evaluados

Se evaluaron dos tipos de prompts: **Point Prompts** que consisten en un punto (x, y) con label de foreground/background, ofreciendo m√≠nima interacci√≥n humana pero siendo ambiguos en regiones complejas; y **Box Prompts** que utilizan bounding boxes [x1, y1, x2, y2], proporcionando menos ambig√ºedad y mejor contexto espacial aunque requieren m√°s informaci√≥n previa.

#### Resultados Zero-shot

El modelo pretrained falla en √°reas que no se distingue bien el agua, la confunde con superficies oscuras como asfalto o sombras. No captura bien inundaciones irregulares.

### Parte 3: Fine-tuning de SAM

#### Estrategia de Fine-tuning

La estrategia consisti√≥ en congelar el Image Encoder (93.7M par√°metros) y el Prompt Encoder para mantener las features generales y la capacidad de procesar prompts, mientras que solo se entren√≥ el Mask Decoder (4.1M par√°metros) para especializarse en detectar agua. Este approach es m√°s eficiente ya que entrenar solo el 4.3% de los par√°metros evita overfitting y preserva la capacidad de generalizaci√≥n del modelo. Adem√°s, se aplic√≥ data augmentation mediante HorizontalFlip, VerticalFlip, Rotate y RandomBrightnessContrast para generar m√°s variabilidad en las im√°genes de entrenamiento con diferentes orientaciones y niveles de brillo.

#### Resultados del Fine-tuning

**Evoluci√≥n durante entrenamiento**:

| Epoch | Train Loss | Train IoU | Val Loss | Val IoU |
|-------|-----------|-----------|----------|---------|
| 1 | 0.4673 | 0.5372 | 0.4267 | **0.6272** |
| 3 | 0.3324 | 0.6407 | 0.3241 | **0.7336** |
| 6 | 0.2858 | 0.6905 | 0.2840 | **0.7389** |
| 10 | 0.2644 | 0.7176 | 0.2692 | **0.7606**|
| 15 | 0.2416 | 0.7272 | 0.2778 | 0.7412 |

‚úÖ Mejor modelo en la Epoch 10 (Val IoU = 0.7606)


### Parte 4: Comparaci√≥n Pretrained vs Fine-tuned

#### M√©tricas Cuantitativas

| M√©trica | Pretrained | Fine-tuned | Mejora |
|---------|-----------|------------|--------|
| **IoU** | 0.5291 | **0.7497** | **+41.68%** |
| **Dice** | 0.6220 | **0.8375** | **+34.65%** |
| **Precision** | 0.8193 | **0.8839** | **+7.89%** |
| **Recall** | 0.5885 | **0.8087** | **+37.41%** |

![Comparaci√≥n de M√©tricas Pretrained vs Fine-tuned](12-imagenes/metrics-comparison.png)

*Gr√°fico de barras comparando las cuatro m√©tricas principales entre modelo pretrained (azul) y fine-tuned (naranja). Se observa mejora consistente en todas las m√©tricas, siendo Recall la que muestra mayor incremento absoluto , seguida de IoU  y Dice.*


#### An√°lisis Cualitativo - Casos de Ejemplo

**Caso 1**:

![Comparaci√≥n Pretrained vs Fine-tuned - Imagen 0](12-imagenes/comparison-image-0.png)
*Comparaci√≥n visual del caso m√°s dram√°tico. Superior: El modelo pretrained no detect√≥ pr√°cticamente nada del agua, mientras que el fine-tuned captura correctamente la mayor parte del √°rea inundada.*


**Caso 2**:

![Comparaci√≥n Pretrained vs Fine-tuned - Imagen 5](12-imagenes/comparison-image-5.png)
*El fine-tuning mejora significativamente la detecci√≥n de boundaries irregulares y √°reas fragmentadas.*

## Reflexi√≥n y An√°lisis

### 1. SAM es un Foundation Model Poderoso pero Requiere Especializaci√≥n

SAM preentrenado en SA-1B tiene capacidad de segmentaci√≥n general muy buena, pero para dominios espec√≠ficos como flood detection es insuficiente. El fine-tuning con solo 80 im√°genes logra una mejora de mas del 40% demostrando la eficiencia del transfer learning, y entrenar √∫nicamente el 4.3% de los par√°metros preserva la generalizaci√≥n mientras especializa el modelo. 
Los foundation models son excelentes puntos de partida, pero la especializaci√≥n es necesaria para aplicaciones cr√≠ticas.

### 2. Prompts: Box > Point para Segmentaci√≥n de √Åreas Irregulares

En este caso particular los box prompts superan a point prompts ya que las areas inundadas son irregulares y estan en varias partes de la imagen. Un solo punto no captura la extensi√≥n completa, pero un box proporciona contexto espacial.


### 3. Recall es M√°s Cr√≠tico que Precision en Disaster Response

Para monitoreo de desastres, los False Negatives (bajo recall) significan √°reas inundadas no detectadas y personas en riesgo sin ayuda, mientras que los False Positives (baja precision) solo generan falsas alarmas con recursos mal asignados pero sin poner vidas en peligro. El fine-tuning mejor√≥ el recall en +37%, reduciendo significativamente el riesgo de perder √°reas cr√≠ticas, lo que demuestra que en aplicaciones de seguridad y respuesta a emergencias, optimizar recall es prioritario sobre precision.

### 4. Eficiencia del Fine-tuning: Solo 80 Im√°genes

Con solo 80 im√°genes de entrenamiento logramos una mejora de mas del 40% sobre el baseline. El modelo ya conoce conceptos fundamentales como edges y boundaries, texturas y colores, y contexto espacial, por lo que solo necesita aprender c√≥mo se ve el agua espec√≠ficamente en contextos de inundaci√≥n. Esto demuestra que para nuevos dominios no se necesita millones de im√°genes si partimos de un foundation model bien entrenado.

### 5. Combined Loss (BCE + Dice) es Efectivo

La combinaci√≥n 50/50 de BCE y Dice funcion√≥ bien, donde BCE optimiza la clasificaci√≥n pixel-wise y Dice optimiza el overlap global, resultando en un balance efectivo entre precisi√≥n local y coherencia global de la m√°scara.

### 6. Aplicaciones M√°s All√° de Flood Detection

Este tipo de modelado es extremadamente util para detectar y segmentar √°reas de inter√©s:

#### Respuesta ante incidentes

En el √°mbito de gesti√≥n de desastres, SAM puede aplicarse no solo a la detecci√≥n de inundaciones (nuestro caso de uso actual), sino tambi√©n a la segmentaci√≥n de √°reas quemadas en im√°genes satelitales para monitoreo de incendios, identificaci√≥n de zonas de deslizamiento de tierra, y mapeo de edificios colapsados tras terremotos, permitiendo una respuesta m√°s r√°pida y coordinada en situaciones de emergencia.

#### Monitoreo ambiental

Para monitoreo ambiental, este tipo de segmentaci√≥n es util para detectar √°reas de tala ilegal en bosques, analizar patrones en corrientes oce√°nicas mediante las texturas y colores del agua, segmentar floraciones de algas t√≥xicas que afectan ecosistemas marinos, y trackear el retroceso de glaciares como indicador del cambio clim√°tico, proporcionando datos cr√≠ticos para la conservaci√≥n y estudios ambientales.

#### Planificaci√≥n urbana

Para planificaci√≥n urbana, la tecnolog√≠a permite mapear √°reas susceptibles a inundaci√≥n para prevenci√≥n de desastres, detectar islas de calor urbanas donde la temperatura es elevada para mejorar dise√±o de espacios p√∫blicos, y cuantificar √°reas verdes en ciudades para evaluar calidad de vida y planificar desarrollo sostenible.

---

## Preguntas sobre la Tarea

### ¬øPor qu√© el pretrained SAM puede fallar en detectar agua en im√°genes de inundaciones efectivamente?

SAM fue entrenado en SA-1B, un dataset general con objetos bien definidos y boundaries claros. El agua en inundaciones presenta desaf√≠os espec√≠ficos que no est√°n bien representados en ese dataset:

- El agua no tiene forma definida, se adapta al terreno
- Confunde al modelo con superficies met√°licas o vidrio
- Agua marr√≥n/turbia se parece a tierra o barro

### ¬øQu√© componentes de SAM decidiste fine-tunear y por qu√©? ¬øPor qu√© congelamos el image encoder?

Solo el Mask Decoder (4.1M par√°metros, 4.3% del total)
Image Encoder (ViT-B, 93.7M params) y Prompt Encoder
Si tuvi√©ramos muchas mas im√°genes, podr√≠amos considerar fine-tunear el encoder tambi√©n.

### ¬øC√≥mo se comparan point prompts vs box prompts en este caso de uso de flood segmentation?

Box prompts son superiores para este dominio. El contexto espacial del box ayuda a SAM a entender la extensi√≥n del √°rea inundada.

### ¬øQu√© mejoras espec√≠ficas observaste despu√©s del fine-tuning?

- Ahora captura bordes complejos del agua correctamente
- Detecta agua marr√≥n/turbia que antes confund√≠a con tierra

### ¬øEste sistema est√° listo para deployment en un sistema de respuesta a desastres? ¬øQu√© falta?

No esta lista para escenarios de respuesta critica, pero puede servir como prueba piloto o MVP en escenarios controlados

Principalmente faltan datos de entrenamiento de distintos escenarios y de diferentes lugares, no es lo mismo uruguay con paisajes de pradera, que el desierto de atacama en chile o la amazonia brasilera.
Tambien faltarian fotos de casos bordes como fotos en la noche, con niebla o lluvia.

Ademas, agregaria testing con organizaciones que tengan experiencia en la respuesta a desastres, que puedan aydar a validar todo el contexto que esta alrededor del caso de uso, como POC esta bien.

### ¬øC√≥mo cambiar√≠a tu approach si tuvieras 10x m√°s datos? ¬øY si tuvieras 10x menos?

Haria fine-tuning de m√°s componentes, y si tuviera menos, buscaria la forma de generar mas datos usando data augmentation, quizas reduciria mas el scope tambien, dependiendo de las imagenes que tenga, deberian ser al menos d algun tipo especifico de inundacion

### ¬øQu√© desaf√≠os espec√≠ficos presenta la segmentaci√≥n de agua en inundaciones?

Desaf√≠os de todo tipo, desde que el agua se esparce de forma irregular, a que en algunos lugares puede ser m√°s transparente y en otras m√°s lodosas, dependiendo del contexto. Son casos muy complicados de llevar a la pr√°ctica porque necesitar√≠amos much√≠simos datos para poder cubrir la mayor cantidad posible de escenarios.

---

## üìö Recursos Adicionales

### Papers y Documentaci√≥n

- [Segment Anything (SAM) Paper](https://arxiv.org/abs/2304.02643)
- [SAM Official Repository](https://github.com/facebookresearch/segment-anything)
- [SAM Documentation](https://segment-anything.com/)
- [Flood Area Segmentation Dataset](https://www.kaggle.com/datasets/faizalkarim/flood-area-segmentation)
- [Fine-tuning SAM Tutorial](https://encord.com/blog/learn-how-to-fine-tune-the-segment-anything-model-sam/)
- [SAM for Medical Imaging](https://arxiv.org/abs/2304.12306)
- [Prompt Engineering for SAM](https://arxiv.org/abs/2306.17400)

### ü§ñ Otros Modelos de Segmentaci√≥n

Modelos alternativos/complementarios para explorar:

**SAM Variants:**
- [MobileSAM](https://github.com/ChaoningZhang/MobileSAM) - SAM optimizado para dispositivos m√≥viles
- [FastSAM](https://github.com/CASIA-IVA-Lab/FastSAM) - 50x m√°s r√°pido que SAM original
- [SAM-HQ](https://github.com/SysCV/sam-hq) - Mayor calidad en detecci√≥n de bordes
- [EfficientSAM](https://github.com/yformer/EfficientSAM) - Optimizado para eficiencia computacional

**Semantic Segmentation:**
- [SegFormer](https://github.com/NVlabs/SegFormer) - Transformer-based segmentation
- [DeepLabV3+](https://github.com/tensorflow/models/tree/master/research/deeplab) - ResNet-based con ASPP
- [U-Net](https://github.com/milesial/Pytorch-UNet) - Arquitectura cl√°sica para segmentaci√≥n m√©dica
- [Mask2Former](https://github.com/facebookresearch/Mask2Former) - Universal segmentation framework

**Instance Segmentation:**
- [Mask R-CNN](https://github.com/matterport/Mask_RCNN) - Cl√°sico robusto y confiable
- [YOLACT](https://github.com/dbolya/yolact) - Real-time instance segmentation
- [SOLOv2](https://github.com/WXinlong/SOLO) - Simple y efectivo

**Panoptic Segmentation:**
- [Panoptic-DeepLab](https://github.com/bowenc0221/panoptic-deeplab) - Semantic + instance unificado
- [EfficientPS](https://github.com/DeepSceneSeg/EfficientPS) - Estado del arte en panoptic segmentation

### üìä Datasets P√∫blicos Recomendados

**Medical Imaging:**

- [Medical Segmentation Decathlon](http://medicaldecathlon.com/) - 10 tareas m√©dicas diferentes
- [ISIC Skin Lesion](https://www.isic-archive.com/) - Detecci√≥n de melanoma y lesiones cut√°neas
- [Chest X-Ray Segmentation](https://www.kaggle.com/datasets/nikhilpandey360/chest-xray-masks-and-labels) - Segmentaci√≥n de pulmones en rayos X

**Satellite & Aerial:**

- [SpaceNet](https://spacenet.ai/) - Building footprints, roads y infraestructura
- [DOTA](https://captain-whu.github.io/DOTA/) - Object detection en im√°genes a√©reas
- [Aerial Semantic Segmentation](https://www.kaggle.com/datasets/humansintheloop/semantic-segmentation-of-aerial-imagery) - Segmentaci√≥n de im√°genes a√©reas

**Industrial & Manufacturing:**

- [MVTec AD](https://www.mvtec.com/company/research/datasets/mvtec-ad) - Defect detection en manufactura
- [Steel Defect Detection](https://www.kaggle.com/c/severstal-steel-defect-detection) - Detecci√≥n de defectos en acero
- [PCB Defect Dataset](https://www.kaggle.com/datasets/akhatova/pcb-defects) - Defectos en placas de circuito impreso

**Natural Images:**

- [COCO-Stuff](https://github.com/nightrome/cocostuff) - 164K im√°genes con scene segmentation
- [ADE20K](https://groups.csail.mit.edu/vision/datasets/ADE20K/) - Scene parsing con 150 clases
- [Cityscapes](https://www.cityscapes-dataset.com/) - Segmentaci√≥n urbana para conducci√≥n aut√≥noma

**Domain-Specific:**

- [Plant Disease Detection](https://www.kaggle.com/datasets/vipoooool/new-plant-diseases-dataset) - Enfermedades en plantas
- [Underwater Trash](https://www.kaggle.com/datasets/ashewale/underwater-trash-detection) - Detecci√≥n de basura submarina
- [Food Segmentation](https://www.kaggle.com/datasets/kmader/food41) - Segmentaci√≥n de alimentos

