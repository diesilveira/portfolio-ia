# TA14 - LLMs con LangChain: Prompting, Plantillas y Salida Estructurada

## Resumen de la Tarea

En esta tarea exploramos el uso de **Large Language Models (LLMs)** a trav√©s de **LangChain**, un framework dise√±ado para simplificar la construcci√≥n de aplicaciones con modelos de lenguaje. El objetivo principal fue dominar t√©cnicas fundamentales de prompting, control de par√°metros, y salidas estructuradas confiables.

### Metodolog√≠a

La metodolog√≠a consisti√≥ en una progresi√≥n incremental desde conceptos fundamentales hasta aplicaciones avanzadas. Comenzamos con la configuraci√≥n b√°sica de `ChatOpenAI` y pruebas de invocaci√≥n simple, para luego experimentar con par√°metros de decodificaci√≥n (`temperature`, `top_p`, `max_tokens`) y analizar su impacto en la generaci√≥n. A continuaci√≥n, exploramos el dise√±o de prompts reutilizables mediante `ChatPromptTemplate` y la composici√≥n de componentes con LCEL (LangChain Expression Language), lo que permiti√≥ construir pipelines declarativos y mantenibles. El siguiente paso fue implementar schemas con Pydantic para garantizar salidas estructuradas confiables, eliminando la necesidad de parseo manual. Finalmente, aplicamos estos conceptos en casos de uso avanzados como few-shot learning para clasificaci√≥n, patrones Map-Reduce para resumir documentos extensos, extracci√≥n de entidades nombradas, y la implementaci√≥n de un sistema RAG (Retrieval-Augmented Generation) b√°sico con FAISS. Durante toda la pr√°ctica utilizamos `gpt-5-mini` de OpenAI, balanceando costo y capacidad para experimentaci√≥n.

## Implementaci√≥n y Resultados

### Parte 1: Configuraci√≥n B√°sica y Primeras Invocaciones

La configuraci√≥n inicial requiri√≥ la instalaci√≥n de dependencias clave (`langchain`, `langchain-openai`, `langsmith`) y la configuraci√≥n de API keys. El primer contacto con el modelo demostr√≥ la simplicidad de LangChain:

El modelo respondi√≥ con precisi√≥n t√©cnica, capturando los conceptos clave (atenci√≥n, paralelizaci√≥n, codificaci√≥n posicional) en una sola frase coherente.

### Parte 2: Experimentaci√≥n con Temperature

La experimentaci√≥n con diferentes valores de `temperature` (0.0, 0.5, 0.9) revel√≥ patrones claros en el comportamiento del modelo:

| Temperature | Caracter√≠sticas |
|-------------|-----------------|
| 0.0 | Determinista, consistente, repetible |
| 0.5 | Balance entre creatividad y coherencia |
| 0.9 | Alta variabilidad, creatividad m√°xima |

Con `temperature=0.0`, el modelo gener√≥ respuestas casi id√©nticas en m√∫ltiples ejecuciones. Con valores m√°s altos, las respuestas mantuvieron la correcci√≥n pero variaron en estilo y estructura.

### Parte 3: Salida Estructurada con Pydantic

Una de las capacidades m√°s poderosas de LangChain es la garant√≠a de salidas estructuradas mediante schemas de Pydantic. Definiendo clases que heredan de `BaseModel` con campos tipados (como `title: str` y `bullets: List[str]`), podemos utilizar el m√©todo `with_structured_output()` para forzar al modelo a devolver JSON que cumpla exactamente con el schema especificado. El resultado es un objeto Pydantic v√°lido con validaci√≥n de tipos garantizada, eliminando completamente la necesidad de parseo manual, manejo de errores de formato, y validaci√≥n de campos.

Por ejemplo, al solicitar un resumen de riesgos de prompt injection, el modelo devuelve:

```json
{
  "title": "Riesgos de 'prompt injection' en LLM apps",
  "bullets": [
    "Exfiltraci√≥n de datos sensibles: un atacante puede inducir al modelo a revelar secretos o informaci√≥n confidencial presente en el contexto o prompts previos.",
    "Manipulaci√≥n del comportamiento y seguridad: instrucciones maliciosas pueden anular salvaguardas y provocar respuestas inseguras, desinformaci√≥n o acciones no deseadas.",
    "Compromiso de integridad y confianza del sistema: permite eludir controles, ejecutar acciones no autorizadas o afectar componentes downstream (APIs, c√≥digo), minando la fiabilidad y seguridad de la aplicaci√≥n."
  ]
}
```

Esta caracter√≠stica es cr√≠tica para aplicaciones de producci√≥n donde la confiabilidad y el contrato de datos son fundamentales, transformando salidas potencialmente impredecibles en estructuras de datos consistentes y verificables.

### Parte 4: Few-Shot Learning

La comparaci√≥n entre zero-shot y few-shot prompting mostr√≥ diferencias sutiles pero importantes:

- **Zero-shot**: Respuestas correctas pero formatos inconsistentes (a veces solo "POS", otras veces "Etiqueta: POS").

- **Few-shot**: Mayor consistencia en formato, pero tambi√©n mayor verbosidad. En un caso espec√≠fico, el modelo agreg√≥ justificaci√≥n no solicitada, evidenciando que los ejemplos pueden "sobre-ense√±ar" el nivel de detalle.

### Parte 5: Map-Reduce para Res√∫menes Largos

Para textos que exceden el contexto √≥ptimo, implementamos un patr√≥n Map-Reduce:

1. **Split**: Divisi√≥n del texto largo en chunks con `RecursiveCharacterTextSplitter` (chunk_size=700, overlap=100)
2. **Map**: Resumen independiente de cada chunk en 2-3 bullets
3. **Reduce**: Consolidaci√≥n de bullets en un resumen coherente final

El resultado fue un resumen conciso de un texto extenso sobre la historia del ajedrez, capturando los puntos clave sin p√©rdida de informaci√≥n cr√≠tica.

### Parte 6: Extracci√≥n de Entidades

La extracci√≥n de entidades nombradas (NER) se simplifica dram√°ticamente con structured output. Definiendo un schema anidado con clases `Entidad` (que contiene tipo y valor) y `ExtractInfo` (que agrupa t√≠tulo, fecha y una lista de entidades), el modelo puede analizar texto libre y extraer informaci√≥n estructurada de forma confiable.

**Ejemplo**: Dado el texto *"OpenAI anunci√≥ una colaboraci√≥n con la Universidad Catolica del Uruguay en Montevideo el 05/11/2025"*, el extractor devuelve:

```text
ExtractInfo(
  titulo='OpenAI anunci√≥ una colaboraci√≥n con la Universidad Catolica del Uruguay en Montevideo',
  fecha='05/11/2025',
  entidades=[
    Entidad(tipo='ORG', valor='OpenAI'),
    Entidad(tipo='ORG', valor='Universidad Catolica del Uruguay'),
    Entidad(tipo='LOC', valor='Montevideo')
  ]
)
```

El modelo identific√≥ correctamente dos organizaciones y una ubicaci√≥n, demostrando capacidad de comprensi√≥n sem√°ntica y categorizaci√≥n sin necesidad de fine-tuning espec√≠fico para la tarea de NER.

### Parte 7: RAG B√°sico con FAISS

La implementaci√≥n de Retrieval-Augmented Generation combin√≥ varios conceptos:

- **Embeddings**: `OpenAIEmbeddings` para representaci√≥n vectorial de documentos
- **Vector Store**: FAISS para indexaci√≥n y b√∫squeda por similitud
- **Retrieval Chain**: Combinaci√≥n de recuperaci√≥n + generaci√≥n con grounding

El sistema RAG respondi√≥ correctamente "RAG combina recuperaci√≥n + generaci√≥n para mejor grounding" al buscar en la base de conocimiento, demostrando que el contexto relevante fue recuperado y utilizado efectivamente.

## Conclusiones

LangChain reduce significativamente el boilerplate necesario para trabajar con LLMs, encapsulando tareas complejas en abstracciones claras como `ChatPromptTemplate`, `with_structured_output` y cadenas LCEL. El par√°metro `temperature` es el m√°s cr√≠tico para controlar el comportamiento, valores bajos o cercanos a 0 son mejores para tareas que requieren respuestas mas deterministas, mientras que valores mas altos son apropiados para generaci√≥n creativa. La salida estructurada con Pydantic esta buena para parsear a JSON directamente, aunque puede fallar.

Para documentos largos, Map-Reduce esta muy bueno, RAG es el patr√≥n arquitect√≥nico m√°s importante para aplicaciones LLM ya que nos permiten usar nuestro propio conocimiento y contexto con LLMs que ya funcionan bien, reduce alucinaciones, y no requiere un re-entrenamiento.

## Observabilidad con LangSmith

![Trace de LangSmith - Retrieval Chain](14-imagenes/langsmith.png)

*Visualizaci√≥n del trace completo del `retrieval_chain` en LangSmith, mostrando cada componente del pipeline RAG con sus tiempos de ejecuci√≥n y costos asociados.*

Durante toda la pr√°ctica utilizamos **LangSmith** para tracing y observabilidad de las cadenas LangChain. Esta herramienta es fundamental para entender qu√© est√° pasando con nuestras aplicaciones LLM, especialmente en cadenas complejas como RAG.

En el trace del retrieval_chain podemos observar la ejecuci√≥n completa del pipeline RAG:

1. **retrieve_documents**: El retriever busca en FAISS los documentos m√°s relevantes usando embeddings sem√°nticos
2. **VectorStoreRetriever**: Ejecuta la b√∫squeda por similitud y devuelve los top-k documentos
3. **stuff_documents_chain**: Toma los documentos recuperados y los agrega al contexto del prompt
4. **ChatOpenAI**: El LLM genera la respuesta final bas√°ndose en el contexto recuperado

El trace muestra que el sistema RAG respondi√≥ correctamente "*RAG combina recuperaci√≥n + generaci√≥n para mejor grounding*" a la pregunta "¬øQu√© ventaja clave aporta RAG?". Los documentos recuperados fueron relevantes (menciones de RAG, OpenAIEmbeddings, y structured output con Pydantic), demostrando que el retrieval funcion√≥ efectivamente.

**M√©tricas clave observadas:**

- Tiempo total: 6.30s
- Costo: $0.0005 por query
- Tokens: 284 tokens totales en la generaci√≥n

LangSmith permite identificar puntos de falla, costos, tiempo, por ejemplo el prompt al chatGPT es lo que demora mas, depurar fallos en el retrieval, y optimizar costos monitoreando el uso de tokens en cada paso de la cadena.

## Preguntas de Reflexi√≥n

### ¬øQu√© cambia si ped√≠s 1 vs 3 oraciones?

Con 3 oraciones, agrega contexto, ejemplos o matices. La diferencia es significativa en densidad de informaci√≥n vs claridad pedag√≥gica.

### ¬øObserv√°s variancia entre ejecuciones con la misma consigna?

Con temperatura baja, pr√°cticamente no varia. Con temperatura mas varia incluso en estructura de respuesta.

### ¬øQu√© combinaci√≥n te da claridad vs creatividad?

- Claridad: `temperature=0.0` + instrucciones espec√≠ficas ("en 2 bullets", "solo hechos")  
- Creatividad: `temperature=0.9` + prompts abiertos ("explora posibilidades", "genera alternativas")

### ¬øCu√°ndo conviene few-shot vs instrucciones claras?

Few-shot conviene principalmente cuando el formato de salida es no est√°ndar, el dominio es t√©cnico y requiere desambiguaci√≥n, o las instrucciones zero-shot fallan consistentemente. Sin embargo, instrucciones claras son generalmente preferibles por ser m√°s eficientes en tokens y m√°s f√°ciles de mantener.

### ¬øC√≥mo cambia el formato cuando el template fija estructura?

Los templates con roles claros (`system`, `human`, `assistant`) mejoran consistencia. El modelo "entiende" su rol y las expectativas, reduciendo respuestas fuera de contexto.

### ¬øQu√© mejora frente a "parsear a mano" cadenas JSON?

Elimina errores de parseo (JSON malformado), validaci√≥n de tipos manual, manejo de campos opcionales o faltantes, y la necesidad de documentar contratos expl√≠citamente ya que el schema sirve como documentaci√≥n.

### ¬øQu√© contratos de salida necesit√°s en producci√≥n?

Toda salida que alimente sistemas downstream debe tener schema Pydantic expl√≠cito con tipos estrictos, validadores de rango y formato, campos obligatorios vs opcionales claramente definidos, y docstrings para cada campo.

### ¬øQu√© prompt te cost√≥ m√°s tokens?

El Map-Reduce sobre texto largo, donde cada chunk se procesa independientemente y luego se consolida. Sin embargo, es inevitable para textos largos si se quiere mantener calidad.

### ¬øC√≥mo balancear latencia vs calidad?

Las opciones principales en orden de impacto son: usar un modelo m√°s r√°pidos dependiendo de la tarea que quieramos hacer, por ejemplo si queremos extraer la idea principal de un texto podemos usar GPT3.5 nano tranquilamente, mientras que para tareas mas complejas vamos a necesitar algun modelo mas poderoso.

### ¬øCu√°ndo "alucina" el modelo al no tener suficiente contexto?

Cuando se le pide informaci√≥n espec√≠fica no presente en el prompt. En el ejemplo Q&A, el modelo correctamente respondi√≥ "No suficiente contexto" cuando el contexto era vago. RAG mitiga esto trayendo documentos relevantes.

### ¬øC√≥mo afectan chunk_size y chunk_overlap la calidad?

Un chunk_size muy peque√±o pierde contexto y genera res√∫menes fragmentados, mientras que uno muy grande resulta costoso y queda con informaci√≥n menos relevante. Un chunk_overlap muy bajo tiene riesgo de cortar oraciones o ideas, mientras que uno muy alto genera redundancia innecesaria y mayor costo.

---

## üìö Referencias

### Documentaci√≥n LangChain

- [Integraci√≥n OpenAI en LangChain (langchain-openai)](https://python.langchain.com/docs/integrations/platforms/openai) ‚Äî instalaci√≥n y uso de ChatOpenAI
- [ChatPromptTemplate (Python API docs)](https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) ‚Äî plantillas de chat como runnables
- [LCEL / Runnables](https://python.langchain.com/docs/concepts/lcel/) ‚Äî composici√≥n con `|`, invoke/batch/stream
- [Structured Output](https://python.langchain.com/docs/how_to/structured_output/) ‚Äî JSON/Pydantic v√°lido con `with_structured_output`
- [LangSmith (tracing/observability)](https://docs.smith.langchain.com/) ‚Äî gu√≠as y quickstart
- [Text Splitters](https://python.langchain.com/docs/how_to/recursive_text_splitter/) ‚Äî dividir documentos para pipelines de resumen
- [Retrieval (RAG)](https://python.langchain.com/docs/tutorials/rag/) ‚Äî cadenas de combinaci√≥n y recuperaci√≥n
- [FAISS VectorStore](https://python.langchain.com/docs/integrations/vectorstores/faiss/) ‚Äî √≠ndice local para RAG
- [LangSmith Evaluation](https://docs.smith.langchain.com/evaluation) ‚Äî datasets y evaluaci√≥n

### Recursos OpenAI

- [OpenAI Text Generation](https://platform.openai.com/docs/guides/text-generation) ‚Äî gu√≠a de par√°metros (temperature, top_p, max_tokens)

### Papers y Recursos Adicionales

- [Prompt Engineering Guide](https://www.promptingguide.ai/) ‚Äî t√©cnicas avanzadas de prompting
- [LangChain Expression Language](https://python.langchain.com/docs/concepts/lcel/) ‚Äî patrones de composici√≥n
- [RAG Survey Paper](https://arxiv.org/abs/2312.10997) ‚Äî estado del arte en Retrieval-Augmented Generation
