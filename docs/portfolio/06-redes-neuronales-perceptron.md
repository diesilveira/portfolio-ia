# TA7 - Redes Neuronales: Del Perceptr√≥n Simple al MLP

## Resumen de la Tarea

La **TA7** se enfoc√≥ en el estudio de redes neuronales artificiales, comenzando con el perceptr√≥n simple y avanzando hacia redes multicapa (MLP). El objetivo principal fue comprender los fundamentos de las redes neuronales, sus limitaciones y capacidades, implementando desde problemas de l√≥gica booleana hasta clasificaci√≥n en datasets reales.

### Metodolog√≠a

1. **Implementaci√≥n del perceptr√≥n simple**: Desarrollo de la funci√≥n b√°sica del perceptr√≥n y visualizaci√≥n de su funcionamiento
2. **Problemas de l√≥gica booleana**: Implementaci√≥n de operadores AND, OR, NOT y XOR
3. **An√°lisis de limitaciones**: Demostraci√≥n de por qu√© el perceptr√≥n simple no puede resolver XOR
4. **Redes multicapa (MLP)**: Implementaci√≥n usando scikit-learn para resolver problemas no linealmente separables
5. **Comparaci√≥n con TensorFlow/Keras**: Implementaci√≥n de una red neuronal profesional
6. **Evaluaci√≥n en dataset real**: Aplicaci√≥n a un problema de clasificaci√≥n m√°s complejo

### Conceptos Clave

- **Perceptr√≥n**: Unidad b√°sica de procesamiento que implementa una funci√≥n de activaci√≥n lineal
- **Funci√≥n de activaci√≥n**: Determina la salida de una neurona basada en sus entradas
- **Separabilidad lineal**: Capacidad de separar clases con una l√≠nea recta (limitaci√≥n del perceptr√≥n simple)
- **MLP**: Red neuronal multicapa capaz de resolver problemas no linealmente separables

## Implementaci√≥n y Resultados

### Perceptr√≥n Simple

```python
def perceptron(x1, x2, w1, w2, bias):
    return 1 if (w1*x1 + w2*x2 + bias) >= 0 else 0
```

**Resultados obtenidos:**

- ‚úÖ **AND**: Resuelto exitosamente con pesos w1=0.5, w2=0.5, bias=-1
- ‚úÖ **OR**: Resuelto exitosamente con pesos w1=0.5, w2=0.5, bias=-0.1
- ‚úÖ **NOT**: Resuelto exitosamente con pesos w1=-1, w2=0, bias=0.5
- ‚ùå **XOR**: Imposible de resolver con perceptr√≥n simple (problema no linealmente separable)

![XOR - Imposible l√≠nea recta](07-imagenes/XOR%20-%20imposible%20linea%20recta.png)

### Red Neuronal Multicapa (MLP)

Para resolver el problema XOR, implementamos una red multicapa:

```python
mlp_xor = MLPClassifier(
    hidden_layer_sizes=(6,),
    activation='relu',
    solver='adam',
    random_state=42,
    max_iter=4000
)
```

**Resultado**: ‚úÖ XOR resuelto con 100% de precisi√≥n

![MLP para XOR](07-imagenes/MLP%20para%20XOR.png)

![Diferencia entre perceptr√≥n y MLP en el c√°lculo de XOR](07-imagenes/diferencia%20entre%20perceptron%20y%20MLP%20en%20el%20calculo%20de%20XOR.png)

### Aplicaci√≥n en Dataset Real

Implementamos un MLP para un dataset de clasificaci√≥n m√°s complejo:

| Modelo | Arquitectura | Training Accuracy | Test Accuracy |
|--------|-------------|-------------------|---------------|
| Scikit-learn MLP | 20 ‚Üí (100, 50) ‚Üí 2 | 100.0% | 93.0% |
| TensorFlow/Keras | 20 ‚Üí (64, 128) ‚Üí 1 | 99.7% | 96.0% |

## Visualizaciones Principales

El an√°lisis incluy√≥ m√∫ltiples visualizaciones:

1. **Separaci√≥n lineal**: Visualizaci√≥n de c√≥mo el perceptr√≥n separa clases con una l√≠nea recta
2. **Problema XOR**: Demostraci√≥n visual de por qu√© XOR no es linealmente separable
3. **Superficies de decisi√≥n**: Comparaci√≥n entre perceptr√≥n simple y MLP
4. **Matrices de confusi√≥n**: Evaluaci√≥n del rendimiento en datasets reales
5. **Curvas de aprendizaje**: An√°lisis del entrenamiento con TensorFlow

## Reflexi√≥n

### Hallazgos Principales

1. **Limitaciones del perceptr√≥n simple**: El perceptr√≥n solo puede resolver problemas linealmente separables, fallando en casos como XOR
2. **Poder de las redes multicapa**: Los MLPs pueden resolver problemas no lineales mediante la combinaci√≥n de m√∫ltiples perceptrones
3. **Importancia de la arquitectura**: El n√∫mero de neuronas ocultas y capas afecta significativamente el rendimiento
4. **Comparaci√≥n de frameworks**: TensorFlow/Keras ofrece mayor flexibilidad y control que scikit-learn

### Desaf√≠os Encontrados

- **Selecci√≥n de hiperpar√°metros**: Encontrar la arquitectura √≥ptima requiere experimentaci√≥n
- **Overfitting**: Los modelos complejos pueden memorizar los datos de entrenamiento
- **Interpretabilidad**: Las redes neuronales son "cajas negras" dif√≠ciles de interpretar
- **Tiempo de entrenamiento**: Los modelos m√°s complejos requieren m√°s tiempo computacional

### Comparaciones y Mejoras

- **Perceptr√≥n vs MLP**: El MLP supera las limitaciones del perceptr√≥n simple pero es m√°s complejo
- **Scikit-learn vs TensorFlow**: TensorFlow ofrece mayor control pero requiere m√°s c√≥digo
- **Arquitecturas diferentes**: M√°s capas y neuronas mejoran la capacidad pero aumentan el riesgo de overfitting

### Preguntas de Reflexi√≥n y Respuestas

**¬øPor qu√© AND, OR y NOT funcionaron pero XOR no?** üìè

**R:** Los operadores AND, OR y NOT son **linealmente separables**, es decir, se pueden resolver trazando una l√≠nea recta que separe las clases. XOR no es linealmente separable porque no existe una l√≠nea recta que pueda separar correctamente los puntos (0,1) y (1,0) de los puntos (0,0) y (1,1). Un perceptr√≥n simple solo puede crear fronteras de decisi√≥n lineales.

**¬øCu√°l es la diferencia clave entre los pesos de AND vs OR?** üéöÔ∏è

**R:** La diferencia est√° en el **umbral (bias)**. AND necesita un umbral m√°s alto (bias=-1) porque requiere que AMBAS entradas sean 1 para activarse. OR tiene un umbral m√°s bajo (bias=-0.1) porque se activa cuando CUALQUIERA de las entradas es 1.

**¬øQu√© otros problemas del mundo real ser√≠an como XOR?** üö¶

**R:** Problemas de **exclusi√≥n mutua** como:

- Clasificar si un n√∫mero es par O impar (pero no ambos)
- Clasificar si un email es spam O leg√≠timo

**¬øPor qu√© sklearn MLP puede resolver XOR pero un perceptr√≥n no?** üß†

**R:** Un perceptr√≥n simple solo puede crear **una l√≠nea de decisi√≥n**. Un MLP con capas ocultas puede crear **m√∫ltiples l√≠neas de decisi√≥n** que se combinan para formar fronteras no lineales. Esto permite resolver problemas como XOR que requieren regiones de decisi√≥n m√°s complejas.

**¬øCu√°l es la principal diferencia entre TensorFlow/Keras y sklearn MLP?** üîß

**R:** **TensorFlow/Keras** ofrece mucho m√°s control sobre el proceso de entrenamiento (epochs, batch_size, callbacks, optimizadores personalizados), mientras que **sklearn MLP** es m√°s simple pero menos flexible. TensorFlow es mejor para investigaci√≥n y modelos complejos, sklearn para prototipos r√°pidos.

**¬øPor qu√© TensorFlow usa epochs y batch_size mientras sklearn MLP no?** ‚öôÔ∏è

**R:** TensorFlow procesa los datos en **mini-batches** (lotes peque√±os), lo que permite manejar datasets grandes y actualizar los pesos gradualmente. Sklearn MLP procesa **todo el dataset junto** en cada iteraci√≥n, lo que es m√°s simple pero menos escalable.

**¬øCu√°ndo usar√≠as sigmoid vs relu como funci√≥n de activaci√≥n?** üìä

**R:**
 ReLU es mejor para capas ocultas porque evita el problema del gradiente que desaparece y es computacionalmente eficiente.
 Sigmoid es mejor para la capa de salida en clasificaci√≥n binaria porque produce valores entre 0 y 1

**¬øQu√© ventaja tiene PyTorch Lightning sobre TensorFlow puro?** üìù

**R:** PyTorch Lightning reduce significativamente el c√≥digo necesario para experimentos. Organiza autom√°ticamente el c√≥digo de entrenamiento, validaci√≥n y testing, maneja la distribuci√≥n en m√∫ltiples GPUs, y proporciona logging autom√°tico.

**¬øPor qu√© PyTorch Lightning separa training_step y test_step?** üîÄ

**R:** Durante el entrenamiento se calculan gradientes y se actualizan pesos, mientras que en evaluaci√≥n solo se hacen predicciones. Lightning separa estos procesos para mayor claridad y para aplicar autom√°ticamente t√©cnicas como dropout solo durante entrenamiento.

**¬øCu√°l framework elegir√≠as para cada escenario?**
Para prototipo r√°pido: sklearn MLP por la simplicidad y rapidez, para un modelo en producci√≥n TensorFlow/Keras o PyTorch Lightning.

**¬øPor qu√© el error "mat1 and mat2 shapes cannot be multiplied" es com√∫n en PyTorch?** üîç

**R:** Este error ocurre cuando las dimensiones no coinciden entre el dataset y la primera capa del modelo. Por ejemplo, si tu dataset tiene 20 caracter√≠sticas pero defines la primera capa con 10 neuronas de entrada.

**¬øQu√© significa el par√°metro deterministic=True en PyTorch Lightning Trainer?** üé≤

**R:** Hace que el entrenamiento sea completamente reproducible eliminando la aleatoriedad. √ötil para investigaci√≥n y debugging, pero puede ser m√°s lento. Sin √©l, cada ejecuci√≥n puede dar resultados ligeramente diferentes.

**¬øPor qu√© TensorFlow muestra curvas de loss y val_loss durante entrenamiento?** üìà

**R:** Para detectar overfitting visualmente. Si el loss de entrenamiento baja pero el de validaci√≥n sube, indica que el modelo est√° memorizando los datos de entrenamiento en lugar de generalizar.

**¬øCu√°l es la diferencia entre trainer.test() y trainer.predict() en PyTorch Lightning?** üéØ

**R:** trainer.test() calcula m√©tricas de evaluaci√≥n (accuracy, precision, recall) y trainer.predict() solo genera predicciones sin calcular m√©tricas

**¬øPor qu√© sklearn MLP es m√°s f√°cil pero menos flexible?** üõ†Ô∏è

**R:** sklearn abstrae muchos detalles t√©cnicos lo que lo hace f√°cil de usar, pero se pierde control fino sobre el proceso de entrenamiento, arquitecturas personalizadas y t√©cnicas avanzadas de regularizaci√≥n.

---

> *"Las redes neuronales nos ense√±an que la complejidad emerge de la simplicidad, y que m√∫ltiples elementos simples pueden resolver problemas que individualmente no podr√≠an abordar"*
