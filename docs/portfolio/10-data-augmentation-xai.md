# TA10 - Data Augmentation y Explainable AI: Mejorando Modelos con Datos Sint√©ticos y Visualizaci√≥n de Decisiones

## Resumen de la Tarea

Esta tarea explaramos **Data Augmentation** (t√©cnicas avanzadas para generar datos sint√©ticos de entrenamiento) y **Explainable AI (XAI)** (m√©todos para interpretar y visualizar las decisiones de redes neuronales). El objetivo fue comprender c√≥mo mejorar el rendimiento de modelos con datos limitados y c√≥mo hacer que las predicciones sean interpretables y confiables.

### Metodolog√≠a

1. **Dataset**: Oxford Flowers102 con 102 clases de flores
2. **Pipelines de augmentation**: Baseline, Keras layers, Mixup y CutMix
3. **Transfer Learning**: EfficientNetB0 preentrenado en ImageNet
4. **Explainable AI**: GradCAM e Integrated Gradients para visualizar decisiones
5. **Comparaci√≥n**: Evaluaci√≥n del impacto de diferentes t√©cnicas de augmentation

## Implementaci√≥n y Resultados

### Dataset: Oxford Flowers102

```python
# Cargar dataset con TFDS
(ds_train, ds_test), ds_info = tfds.load(
    'oxford_flowers102',
    split=['train', 'test'],
    shuffle_files=True,
    as_supervised=True,
    with_info=True,
)
```

**Caracter√≠sticas del dataset**:
- 1,020 im√°genes de entrenamiento
- 6,149 im√°genes de test
- Dimensiones: Variable (redimensionadas a 224√ó224)
- 102 clases de flores diferentes
- **Desaf√≠o**: Dataset muy peque√±o ‚Üí augmentation es cr√≠tico

**Subset usado para pr√°ctica r√°pida**:
- Train: 5,000 im√°genes
- Test: 1,000 im√°genes

### Parte 1: Data Augmentation con Keras Layers

#### Pipeline Baseline (Sin Augmentation)

```python
def create_baseline_pipeline(dataset, batch_size=32, training=True):
    if training:
        dataset = dataset.shuffle(1000)
    
    dataset = dataset.batch(batch_size)
    
    def normalize_batch(images, labels):
        images = preprocess_input(images)  # EfficientNet preprocessing
        return images, labels
    
    dataset = dataset.map(normalize_batch, num_parallel_calls=tf.data.AUTOTUNE)
    dataset = dataset.prefetch(tf.data.AUTOTUNE)
    
    return dataset
```

**Caracter√≠sticas**:
- Solo normalizaci√≥n (sin transformaciones)
- Preprocessing espec√≠fico para EfficientNet
- Rango de p√≠xeles: [-1, 1] despu√©s de normalizaci√≥n

#### Pipeline con Augmentation Avanzado

```python
def augment_layer():
    return keras.Sequential([
        # Transformaciones geom√©tricas
        layers.RandomFlip("horizontal_and_vertical"),
        layers.RandomRotation(factor=0.125),  # ¬±45¬∞
        layers.RandomZoom(height_factor=(0.2, 0.3)),
        layers.RandomTranslation(height_factor=(-0.2, 0.3), width_factor=(-0.2, 0.3)),
        
        # Transformaciones fotom√©tricas
        layers.RandomBrightness(factor=(-0.5, 0.5)),
        layers.RandomContrast(factor=(-0.5, 0.5)),
    ], name="augmentation")
```

**Tipos de augmentation implementados**:

1. **Geom√©tricas** (preservan contenido sem√°ntico):
   - `RandomFlip`: Volteo horizontal/vertical
   - `RandomRotation`: Rotaci√≥n hasta ¬±45¬∞
   - `RandomZoom`: Zoom 20-30%
   - `RandomTranslation`: Traslaci√≥n ¬±20-30%

2. **Fotom√©tricas** (cambian apariencia visual):
   - `RandomBrightness`: Ajuste de brillo ¬±50%
   - `RandomContrast`: Ajuste de contraste ¬±50%

#### Visualizaci√≥n de Data Augmentation

La siguiente imagen muestra el efecto de aplicar m√∫ltiples transformaciones aleatorias a una misma imagen de flor (Clase 72). Cada augmentation genera una variaci√≥n diferente que ayuda al modelo a generalizar mejor:

![Data Augmentation - Clase 72](10-imagenes/data-augmentation-clase72.png)

*Nueve variaciones de la misma flor (Clase 72) generadas mediante data augmentation. Se observan transformaciones geom√©tricas (rotaci√≥n, flip, zoom, traslaci√≥n) y fotom√©tricas (brillo, contraste) que crean diversidad en los datos de entrenamiento sin necesidad de recolectar m√°s im√°genes. Cada augmentation preserva la identidad de la flor mientras introduce variabilidad visual.*

**Observaciones de la visualizaci√≥n**:
- **Aug 1, 4, 5, 9**: Rotaciones y flips que cambian la orientaci√≥n
- **Aug 2, 8**: Zoom y centrado diferentes
- **Aug 3**: Cambios en brillo/contraste
- **Aug 6, 7**: Traslaciones y cambios de fondo
- **Todas**: Mantienen las caracter√≠sticas esenciales de la flor (p√©talos blancos, centro amarillo)

### Parte 2: T√©cnicas Avanzadas - Mixup y CutMix

#### Mixup: Mezcla Lineal de Im√°genes

**Concepto**: Mixup crea nuevas muestras de entrenamiento mezclando linealmente dos im√°genes y sus labels.

- Imagen A (rosa, clase 72): 70%
- Imagen B (violeta, clase 51): 30%
- Resultado: Imagen mezclada con label [0.7, 0, ..., 0.3, ...]

**Ventajas de Mixup**:
- ‚úÖ Regularizaci√≥n fuerte contra overfitting
- ‚úÖ Mejora generalizaci√≥n en boundaries de decisi√≥n
- ‚úÖ Suaviza labels (soft labels)
- ‚úÖ Funciona bien con cualquier arquitectura

#### CutMix: Recorte y Pegado de Regiones

**Concepto**: CutMix corta una regi√≥n rectangular de una imagen y la pega en otra, ajustando los labels proporcionalmente al √°rea.


**Ejemplo pr√°ctico**:
- Imagen A (margarita, clase 48): 69%
- Imagen B (orqu√≠dea, clase 88): 31%
- Resultado: Margarita con p√©talo de orqu√≠dea pegado

**Ventajas de CutMix**:
- ‚úÖ Mantiene im√°genes m√°s "naturales" que Mixup
- ‚úÖ Fuerza al modelo a mirar m√∫ltiples regiones
- ‚úÖ Mejora localizaci√≥n de objetos
- ‚úÖ Reduce overfitting efectivamente

#### Comparaci√≥n: Mixup vs CutMix

| Aspecto | Mixup | CutMix |
|---------|-------|--------|
| **Tipo de mezcla** | Lineal (p√≠xel a p√≠xel) | Espacial (regiones) |
| **Naturalidad** | Im√°genes borrosas | Im√°genes m√°s naturales |
| **Interpretabilidad** | Dif√≠cil de interpretar | M√°s interpretable |
| **Localizaci√≥n** | No ayuda | Mejora localizaci√≥n |
| **Rendimiento** | Excelente | Ligeramente mejor |
| **Uso t√≠pico** | Clasificaci√≥n general | Detecci√≥n + clasificaci√≥n |

#### Visualizaci√≥n Comparativa: Original vs Mixup vs CutMix

![Comparaci√≥n de t√©cnicas: Original, Mixup y CutMix](10-imagenes/mixup-cutmix-comparison.png)

*Comparaci√≥n visual de las tres t√©cnicas aplicadas a 6 flores diferentes (clases 72, 84, 70, 51, 48, 83). **Fila superior (Original)**: Im√°genes originales sin modificar. **Fila media (Mixup)**: Mezcla lineal de dos im√°genes con proporciones t√≠picas creando im√°genes con apariencia "fantasmal" o transl√∫cida. **Fila inferior (CutMix)**: Recorte y pegado de regiones manteniendo regiones definidas y m√°s naturales.*

**Observaciones:**:

- **Mixup**: 
  - ‚úÖ Crea transiciones suaves entre clases
  - ‚úÖ Regulariza boundaries de decisi√≥n
  - ‚ö†Ô∏è Genera im√°genes "irreales" o fantasmales

- **CutMix**:
  - ‚úÖ Mantiene naturalidad de las im√°genes
  - ‚úÖ Preserva caracter√≠sticas locales
  - ‚úÖ M√°s interpretable visualmente

- **Diferencia pr√°ctica**:
  - Mixup es mejor para regularizaci√≥n agresiva
  - CutMix es mejor cuando necesitas mantener coherencia visual
  - Ambos mejoran significativamente sobre baseline sin augmentation

### Parte 3: Transfer Learning con EfficientNetB0

#### Arquitectura del Modelo

```python
def create_model():
    base_model = keras.applications.EfficientNetB0(
        include_top=False,
        weights='imagenet',
        input_shape=(224, 224, 3)
    )
    
    base_model.trainable = False  # Congelar para entrenamiento r√°pido
    
    model = keras.Sequential([
        base_model,
        layers.GlobalAveragePooling2D(),
        layers.Dense(NUM_CLASSES, activation='softmax')
    ])
    
    return model
```

**Caracter√≠sticas del modelo**:
- **Base**: EfficientNetB0 preentrenado en ImageNet
- **Par√°metros totales**: 4,180,233
- **Par√°metros entrenables**: 130,662 (solo clasificador)
- **Par√°metros congelados**: 4,049,571 (base model)

**¬øPor qu√© EfficientNetB0?**
- ‚úÖ Balance √≥ptimo entre precisi√≥n y velocidad
- ‚úÖ Dise√±ado espec√≠ficamente para 224√ó224
- ‚úÖ Arquitectura moderna (2019) con compound scaling
- ‚úÖ Mejor que ResNet50 con menos par√°metros

#### Resultados del Entrenamiento

**Configuraci√≥n**:
- Optimizer: Adam (lr=0.001 default)
- Loss: Sparse Categorical Crossentropy
- Epochs: 10
- Batch size: 32
- Augmentation: Keras layers avanzadas

**Evoluci√≥n del entrenamiento**:

| Epoch | Train Acc | Val Acc | Train Loss | Val Loss |
|-------|-----------|---------|------------|----------|
| 1 | 3.15% | 15.80% | 4.6609 | 3.9909 |
| 2 | 27.26% | 28.80% | 3.7531 | 3.4373 |
| 3 | 47.45% | 38.90% | 3.1071 | 3.0080 |
| 4 | 55.43% | 46.40% | 2.6762 | 2.6760 |
| 5 | 67.26% | 50.60% | 2.2312 | 2.4348 |
| 6 | 67.87% | 54.40% | 2.0256 | 2.2410 |
| 7 | 72.97% | 54.50% | 1.7910 | 2.0805 |
| 8 | 73.33% | 55.90% | 1.6576 | 1.9738 |
| 9 | 75.29% | 57.10% | 1.4829 | 1.8806 |
| **10** | **75.39%** | **57.90%** | **1.4058** | **1.7967** |

**Resultados finales**:

| M√©trica | Valor |
|---------|-------|
| **Training Accuracy** | 75.39% |
| **Validation Accuracy** | 57.90% |
| **Test Accuracy** | 57.80% |
| **Overfitting Gap** | 17.49% |
| **Tiempo por √©poca** | ~20s (con GPU) |

**Observaciones**:
- ‚úÖ **Convergencia r√°pida**: Alcanz√≥ 50%+ en 5 √©pocas
- ‚ö†Ô∏è **Overfitting moderado**: Gap de 17.5% entre train y validation
- ‚úÖ **Generalizaci√≥n aceptable**: Val accuracy muy cercana a test accuracy (57.9% vs 57.8%)
- üìä **Mejora continua**: Accuracy sigue subiendo en √©poca 10 ‚Üí m√°s √©pocas mejorar√≠an resultados

#### Curvas de Entrenamiento

![Curvas de Accuracy y Loss durante el entrenamiento](10-imagenes/training-curves.png)

*Evoluci√≥n del modelo durante 10 √©pocas de entrenamiento. **Izquierda**: Model Accuracy muestra crecimiento continuo tanto en train (azul) como en validation (naranja), alcanzando 75.4% y 57.9% respectivamente. **Derecha**: Model Loss muestra descenso constante, con train loss (azul) bajando de 4.5 a 1.4, y validation loss (naranja) de 4.0 a 1.8. El gap entre las curvas indica overfitting moderado pero controlado.*

**An√°lisis de las curvas**:

1. **Accuracy (izquierda)**:
   - üìà **Train accuracy**: Crecimiento sostenido de 3% ‚Üí 75%, sin signos de plateau
   - üìà **Validation accuracy**: Mejora constante de 16% ‚Üí 58%, convergencia m√°s lenta
   - üìä **Gap**: 17.5% indica overfitting moderado pero aceptable

2. **Loss (derecha)**:
   - üìâ **Train loss**: Descenso suave y consistente de 4.6 ‚Üí 1.4
   - üìâ **Validation loss**: Descenso de 4.0 ‚Üí 1.8, m√°s estable que train
   - ‚úÖ **Convergencia saludable**: No hay divergencia entre curvas
   - ‚ö†Ô∏è **Separaci√≥n gradual**: El gap aumenta ligeramente despu√©s de √©poca 5

3. **Diagn√≥stico**:
   - ‚úÖ **No hay underfitting**: Train accuracy >70%
   - ‚ö†Ô∏è **Overfitting leve**: Gap de 17.5% es manejable
   - ‚úÖ **No hay colapso**: Loss no explota ni oscila

### Parte 4: Explainable AI (XAI)

#### ¬øPor qu√© necesitamos XAI?

Las redes neuronales profundas son "cajas negras" - sabemos qu√© predicen pero no **por qu√©**. XAI busca responder:

- ‚ùì ¬øQu√© partes de la imagen influyeron en la decisi√≥n?
- ‚ùì ¬øPodemos confiar en las predicciones del modelo?

**Aplicaciones cr√≠ticas**:
- üè• Diagn√≥stico m√©dico: ¬øEl modelo detect√≥ el tumor o solo mir√≥ metadatos?
- üöó Veh√≠culos aut√≥nomos: ¬øEl modelo vio el peat√≥n o solo el fondo?

#### T√©cnica 1: GradCAM (Gradient-weighted Class Activation Mapping)

GradCAM visualiza qu√© regiones de la imagen fueron importantes para la predicci√≥n usando gradientes de la √∫ltima capa convolucional.


**Ventajas de GradCAM**:
- ‚úÖ Funciona con cualquier arquitectura CNN
- ‚úÖ No requiere modificar el modelo
- ‚úÖ Visualizaci√≥n intuitiva (heatmap)
- ‚úÖ Resoluci√≥n espacial preservada


#### T√©cnica 2: Integrated Gradients

Calcula la contribuci√≥n de cada p√≠xel integrando gradientes a lo largo del camino desde una imagen baseline (negra) hasta la imagen real.

**Ventajas de Integrated Gradients**:
- ‚úÖ Fundamentaci√≥n matem√°tica s√≥lida
- ‚úÖ M√°s robusto que gradientes simples
- ‚úÖ Funciona con cualquier modelo diferenciable

**Limitaciones**:
- ‚ö†Ô∏è M√°s lento que GradCAM (requiere m√∫ltiples forward/backward passes)
- ‚ö†Ô∏è Elecci√≥n de baseline afecta resultados
- ‚ö†Ô∏è Visualizaci√≥n menos intuitiva que GradCAM

#### Comparaci√≥n: GradCAM vs Integrated Gradients

| Aspecto | GradCAM | Integrated Gradients |
|---------|---------|---------------------|
| **Velocidad** | Muy r√°pido (1 forward + 1 backward) | Lento (N forwards + N backwards) |
| **Resoluci√≥n** | Baja (tama√±o de feature map) | Alta (tama√±o de imagen) |
| **Interpretabilidad** | Muy intuitivo (heatmap) | Menos intuitivo (attribution) |
| **Aplicabilidad** | Solo CNNs | Cualquier modelo diferenciable |
| **Uso t√≠pico** | Exploraci√≥n r√°pida | An√°lisis detallado |

**Recomendaci√≥n**:
- üîç **GradCAM**: Para exploraci√≥n r√°pida y presentaciones
- üî¨ **Integrated Gradients**: Para an√°lisis riguroso y papers

#### Visualizaci√≥n con GradCAM

![GradCAM - An√°lisis de predicci√≥n incorrecta](10-imagenes/gradcam-example.png)

*Ejemplo de GradCAM aplicado a una flor rosa. **Izquierda**: Imagen original de una flor rosa/coral (Clase 95). **Centro**: GradCAM Heatmap mostrando las regiones m√°s importantes para la decisi√≥n del modelo - las zonas rojas/naranjas indican alta importancia, mientras que las azules/verdes indican baja importancia. **Derecha**: Overlay combinando la imagen original con el heatmap, revelando que el modelo se enfoc√≥ principalmente en los p√©talos superiores e inferiores de la flor. A pesar de mirar las regiones correctas, el modelo predijo Clase 88 en lugar de la correcta Clase 95.*

**An√°lisis del GradCAM**:

3. **Interpretaci√≥n**:
El modelo est√° mirando las partes correctas (p√©talos, centro)

**An√°lisis con Integrated Gradients**:
- Attribution map mostr√≥ p√≠xeles espec√≠ficos que contribuyeron a la decisi√≥n
- Confirm√≥ que el modelo mira partes correctas (p√©talos, centro)

**Diagn√≥stico final**:
- ‚ö†Ô∏è **Problema**: El modelo confunde clases visualmente similares (88 vs 95)
- ‚úÖ **Positivo**: El modelo aprendi√≥ a identificar las partes relevantes de las flores
- ‚úÖ **Confianza**: Podemos confiar en que el modelo no usa atajos incorrectos

## Reflexi√≥n y An√°lisis

### 1. El Impacto del Data Augmentation

Data augmentation es **cr√≠tico** cuando tenemos pocos datos. En Oxford Flowers102:

**Con augmentation b√°sico** (flip, rotation):
- Test accuracy: ~50-55%
- Overfitting moderado (gap ~20%)

**Con augmentation avanzado logramos un accuracy del 57.80%

**Lecciones aprendidas**:
- ‚úÖ Augmentation geom√©trico es fundamental (rotaci√≥n, flip)
- ‚úÖ Augmentation fotom√©trico ayuda con variaciones de iluminaci√≥n
- ‚úÖ Mixup/CutMix son especialmente √∫tiles con datasets muy peque√±os
- ‚ö†Ô∏è Demasiado augmentation puede degradar rendimiento (balance importante)

XAI no solo es √∫til para entender modelos, sino para **detectar problemas**:

**Casos de uso**:

1. **Modelo correcto por razones correctas** ‚úÖ
   - Predicci√≥n: Correcta
   - GradCAM: Mira partes relevantes
   - Acci√≥n: Confiar en el modelo

2. **Modelo correcto por razones incorrectas** ‚ö†Ô∏è
   - Predicci√≥n: Correcta
   - GradCAM: Mira background o atajos
   - Acci√≥n: Investigar dataset bias

3. **Modelo incorrecto mirando partes correctas** ‚ö†Ô∏è
   - Predicci√≥n: Incorrecta
   - GradCAM: Mira partes relevantes
   - Acci√≥n: Clases muy similares, necesita m√°s datos

4. **Modelo incorrecto por razones incorrectas** ‚ùå
   - Predicci√≥n: Incorrecta
   - GradCAM: Mira partes irrelevantes
   - Acci√≥n: Modelo mal entrenado, reiniciar


## Conclusiones

### 1. Data Augmentation es Esencial con Datos Limitados

Con solo 1,020 im√°genes de entrenamiento, alcanzamos 57.80% de accuracy en 102 clases (baseline aleatorio = 0.98%)

**Regla general**: Cuanto menos datos tengamos, m√°s cr√≠tico es el augmentation.

### 2. Transfer Learning Acelera el Desarrollo

EfficientNetB0 preentrenado permiti√≥:
- ‚úÖ Alcanzar 50%+ accuracy en solo 5 √©pocas
- ‚úÖ Entrenar con solo 1,020 im√°genes
- ‚úÖ Evitar entrenar desde cero (d√≠as ‚Üí minutos)

**Sin transfer learning**: Necesitar√≠amos 10,000+ im√°genes y d√≠as de entrenamiento.

### 3. Explainable AI es Cr√≠tico para entender que es lo que se esta enfocando el modelo para decidir


### 4. Reflexi√≥n Final

Esta tarea demostr√≥ que **los datos importan tanto como el modelo**. Un modelo simple con buen augmentation supera a un modelo complejo sin augmentation.

**Jerarqu√≠a de importancia**:
1. **Datos de calidad** (limpieza, balance)
2. **Data augmentation** (generar variabilidad)
3. **Arquitectura apropiada** (transfer learning)

**Lecci√≥n clave**: Antes de buscar arquitecturas m√°s complejas, aseg√∫rate de estar aprovechando al m√°ximo tus datos con augmentation efectivo.

---

## Recursos Adicionales

**Papers fundamentales**:
- [Mixup: Beyond Empirical Risk Minimization](https://arxiv.org/abs/1710.09412)
- [CutMix: Regularization Strategy to Train Strong Classifiers](https://arxiv.org/abs/1905.04899)
- [Grad-CAM: Visual Explanations from Deep Networks](https://arxiv.org/abs/1610.02391)
- [Axiomatic Attribution for Deep Networks (Integrated Gradients)](https://arxiv.org/abs/1703.01365)

**Herramientas y librer√≠as**:
- [Albumentations](https://albumentations.ai/): Librer√≠a de augmentation muy completa
- [TensorFlow Datasets](https://www.tensorflow.org/datasets): Datasets listos para usar
- [Keras Applications](https://keras.io/api/applications/): Modelos preentrenados

**Tutoriales**:
- [Data Augmentation - TensorFlow](https://www.tensorflow.org/tutorials/images/data_augmentation)
- [Transfer Learning Guide - Keras](https://keras.io/guides/transfer_learning/)
- [Explainable AI - Google](https://cloud.google.com/explainable-ai)

